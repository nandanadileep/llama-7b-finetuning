{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/nandanadileep/llama-7b-finetuning.git\n%cd llama-7b-finetuning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:54:02.114831Z","iopub.execute_input":"2026-01-02T11:54:02.115636Z","iopub.status.idle":"2026-01-02T11:54:04.144972Z","shell.execute_reply.started":"2026-01-02T11:54:02.115599Z","shell.execute_reply":"2026-01-02T11:54:04.144218Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama-7b-finetuning'...\nremote: Enumerating objects: 77, done.\u001b[K\nremote: Counting objects: 100% (77/77), done.\u001b[K\nremote: Compressing objects: 100% (52/52), done.\u001b[K\nremote: Total 77 (delta 40), reused 51 (delta 17), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (77/77), 14.93 MiB | 27.70 MiB/s, done.\nResolving deltas: 100% (40/40), done.\n/kaggle/working/llama-7b-finetuning\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:54:06.366811Z","iopub.execute_input":"2026-01-02T11:54:06.367406Z","iopub.status.idle":"2026-01-02T11:54:12.773273Z","shell.execute_reply.started":"2026-01-02T11:54:06.367370Z","shell.execute_reply":"2026-01-02T11:54:12.772543Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\nRequirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.57.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.4.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.11.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.17.1)\nCollecting bitsandbytes (from -r requirements.txt (line 6))\n  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting trl (from -r requirements.txt (line 7))\n  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.2.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (25.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.38.0->-r requirements.txt (line 2)) (4.67.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (2.2.2)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 3)) (0.70.18)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 3)) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 3)) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.38.0->-r requirements.txt (line 2)) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.38.0->-r requirements.txt (line 2)) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.38.0->-r requirements.txt (line 2)) (2.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.17.0)\nDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, trl\nSuccessfully installed bitsandbytes-0.49.0 trl-0.26.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls /kaggle/working/llama-7b-finetuning/adapters/final_adapter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:56:14.810188Z","iopub.execute_input":"2026-01-02T11:56:14.810824Z","iopub.status.idle":"2026-01-02T11:56:14.925494Z","shell.execute_reply.started":"2026-01-02T11:56:14.810793Z","shell.execute_reply":"2026-01-02T11:56:14.924886Z"}},"outputs":[{"name":"stdout","text":"adapter_config.json\t   special_tokens_map.json  tokenizer.json\nadapter_model.safetensors  tokenizer_config.json    tokenizer.model\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\nADAPTER_PATH = \"/kaggle/working/llama-7b-finetuning/adapters/final_adapter\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:56:57.611822Z","iopub.execute_input":"2026-01-02T11:56:57.612171Z","iopub.status.idle":"2026-01-02T11:56:57.616193Z","shell.execute_reply.started":"2026-01-02T11:56:57.612140Z","shell.execute_reply":"2026-01-02T11:56:57.615569Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:58:05.578400Z","iopub.execute_input":"2026-01-02T11:58:05.578704Z","iopub.status.idle":"2026-01-02T11:58:05.844023Z","shell.execute_reply.started":"2026-01-02T11:58:05.578675Z","shell.execute_reply":"2026-01-02T11:58:05.843376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff43ec1e025442099ac842546e36777e"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nBASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n\n# tokenizer (use adapter tokenizer first, fallback to base)\ntokenizer = AutoTokenizer.from_pretrained(\n    ADAPTER_PATH,\n    trust_remote_code=True\n)\n\n# base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# load LoRA adapter\nmodel = PeftModel.from_pretrained(\n    model,\n    ADAPTER_PATH\n)\n\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:58:11.971884Z","iopub.execute_input":"2026-01-02T11:58:11.972434Z","iopub.status.idle":"2026-01-02T12:00:58.648275Z","shell.execute_reply.started":"2026-01-02T11:58:11.972405Z","shell.execute_reply":"2026-01-02T12:00:58.647620Z"}},"outputs":[{"name":"stderr","text":"2026-01-02 11:58:30.026970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767355110.250869      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767355110.316236      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767355110.840972      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767355110.841017      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767355110.841020      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767355110.841022      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0061c5d9ef64fa9b4d9c37bdf31a0f8"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2bf69b81f6443f39da91686e37cd71f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01eb7216d7314f07a16c7af87100af05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a622223c27484aa71e8c37eab61189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b17e09dc604368b61f11ab129bff2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8868e66e5744d39f7159713d7a6d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6ba5fb925f418098974e98e19ea076"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def generate(prompt, max_new_tokens=256):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:02:17.686144Z","iopub.execute_input":"2026-01-02T12:02:17.687289Z","iopub.status.idle":"2026-01-02T12:02:17.692031Z","shell.execute_reply.started":"2026-01-02T12:02:17.687241Z","shell.execute_reply":"2026-01-02T12:02:17.691382Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(\n    generate(\n        \"### Instruction:\\nGive me healthy diet tips\\n\\n### Response:\"\n    )\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:02:20.933646Z","iopub.execute_input":"2026-01-02T12:02:20.934044Z","iopub.status.idle":"2026-01-02T12:02:37.929526Z","shell.execute_reply.started":"2026-01-02T12:02:20.934011Z","shell.execute_reply":"2026-01-02T12:02:37.928816Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\nGive me healthy diet tips\n\n### Response:\nCertainly! Here are some healthy diet tips:\n\n1. Eat a variety of fruits and vegetables: Aim to include a variety of colors on your plate to ensure you are getting a range of nutrients.\n2. Incorporate lean protein sources: Choose lean protein sources like chicken, fish, tofu, and legumes to help build and repair muscles.\n3. Whole grains are a must: Choose whole grains like brown rice, quinoa, and whole wheat bread to increase fiber and nutrient intake.\n4. Limit processed foods: Try to limit your intake of processed foods like sugary snacks, frozen meals, and packaged desserts.\n5. Drink plenty of water: Aim to drink at least 8-10 glasses of water per day to stay hydrated.\n6. Be mindful of portion sizes: Pay attention to the serving sizes of foods and beverages to avoid overeating or consuming too many calories.\n7. Incorporate healthy fats: Choose healthy fats like avocado,\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:02:45.370826Z","iopub.execute_input":"2026-01-02T12:02:45.371428Z","iopub.status.idle":"2026-01-02T12:02:51.366546Z","shell.execute_reply.started":"2026-01-02T12:02:45.371396Z","shell.execute_reply":"2026-01-02T12:02:51.365576Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import gradio as gr\n\ndef chat(prompt):\n    return generate(prompt)\n\ngr.Interface(\n    fn=chat,\n    inputs=gr.Textbox(lines=5, label=\"Prompt\"),\n    outputs=gr.Markdown(),\n    title=\"LoRA Fine-Tuned LLaMA Chat\"\n).launch(share=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:11:38.082006Z","iopub.execute_input":"2026-01-02T12:11:38.082917Z","iopub.status.idle":"2026-01-02T12:11:38.706881Z","shell.execute_reply.started":"2026-01-02T12:11:38.082883Z","shell.execute_reply":"2026-01-02T12:11:38.706011Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://850112f696ac821ad4.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://850112f696ac821ad4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"Created dataset file at: .gradio/flagged/dataset1.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}